{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"a192xmwKijHs","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFilter\nfrom tqdm import tqdm_notebook \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss, confusion_matrix\nimport matplotlib.pyplot as plt\n\nnp.random.seed(100)\nLEVEL = 'level_1'","execution_count":null,"outputs":[]},{"metadata":{"id":"fIuRdSezijHx","trusted":true},"cell_type":"code","source":"class SigmoidNeuron:\n  \n  def __init__(self):\n    self.w = None\n    self.b = None\n    \n  def perceptron(self, x):\n    return np.dot(x, self.w.T) + self.b\n  \n  def sigmoid(self, x):\n    return 1.0/(1.0 + np.exp(-x))\n  \n  def grad_w_mse(self, x, y):\n    y_pred = self.sigmoid(self.perceptron(x))\n    return (y_pred - y) * y_pred * (1 - y_pred) * x\n  \n  def grad_b_mse(self, x, y):\n    y_pred = self.sigmoid(self.perceptron(x))\n    return (y_pred - y) * y_pred * (1 - y_pred)\n  \n  def grad_w_ce(self, x, y):\n    y_pred = self.sigmoid(self.perceptron(x))\n    if y == 0:\n      return y_pred * x\n    elif y == 1:\n      return -1 * (1 - y_pred) * x\n    else:\n      raise ValueError(\"y should be 0 or 1\")\n    \n  def grad_b_ce(self, x, y):\n    y_pred = self.sigmoid(self.perceptron(x))\n    if y == 0:\n      return y_pred \n    elif y == 1:\n      return -1 * (1 - y_pred)\n    else:\n      raise ValueError(\"y should be 0 or 1\")\n  \n  def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, loss_fn=\"mse\", display_loss=False):\n    \n    # initialise w, b\n    if initialise:\n      self.w = np.random.randn(1, X.shape[1])\n      self.b = 0\n      \n    if display_loss:\n      loss = {}\n    \n    for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n      dw = 0\n      db = 0\n      for x, y in zip(X, Y):\n        if loss_fn == \"mse\":\n          dw += self.grad_w_mse(x, y)\n          db += self.grad_b_mse(x, y) \n        elif loss_fn == \"ce\":\n          dw += self.grad_w_ce(x, y)\n          db += self.grad_b_ce(x, y)\n      self.w -= learning_rate * dw\n      self.b -= learning_rate * db\n      \n      if display_loss:\n        Y_pred = self.sigmoid(self.perceptron(X))\n        if loss_fn == \"mse\":\n          loss[i] = mean_squared_error(Y, Y_pred)\n        elif loss_fn == \"ce\":\n          loss[i] = log_loss(Y, Y_pred)\n    \n    if display_loss:\n      plt.plot(loss.values())\n      plt.xlabel('Epochs')\n      if loss_fn == \"mse\":\n        plt.ylabel('Mean Squared Error')\n      elif loss_fn == \"ce\":\n        plt.ylabel('Log Loss')\n      plt.show()\n      \n  def predict(self, X):\n    Y_pred = []\n    for x in X:\n      y_pred = self.sigmoid(self.perceptron(x))\n      Y_pred.append(y_pred)\n    return np.array(Y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FF_MultiClass_InputWeightVectorised:\n  \n  def __init__(self, W1, W2):\n    self.W1 = W1.copy()\n    self.W2 = W2.copy()\n    self.B1 = np.zeros((1,2))\n    self.B2 = np.zeros((1,4))\n  \n  def sigmoid(self, X):\n    return 1.0/(1.0 + np.exp(-X))\n  \n  def softmax(self, X):\n    exps = np.exp(X)\n    return exps / np.sum(exps, axis=1).reshape(-1,1)\n  \n  def forward_pass(self, X):\n    self.A1 = np.matmul(X,self.W1) + self.B1 # (N, 2) * (2, 2) -> (N, 2)\n    self.H1 = self.sigmoid(self.A1) # (N, 2)\n    self.A2 = np.matmul(self.H1, self.W2) + self.B2 # (N, 2) * (2, 4) -> (N, 4)\n    self.H2 = self.softmax(self.A2) # (N, 4)\n    return self.H2\n    \n  def grad_sigmoid(self, X):\n    return X*(1-X) \n  \n  def grad(self, X, Y):\n    self.forward_pass(X)\n    m = X.shape[0]\n    \n    self.dA2 = self.H2 - Y # (N, 4) - (N, 4) -> (N, 4)\n    \n    self.dW2 = np.matmul(self.H1.T, self.dA2) # (2, N) * (N, 4) -> (2, 4)\n    self.dB2 = np.sum(self.dA2, axis=0).reshape(1, -1) # (N, 4) -> (1, 4)\n    self.dH1 = np.matmul(self.dA2, self.W2.T) # (N, 4) * (4, 2) -> (N, 2)\n    self.dA1 = np.multiply(self.dH1, self.grad_sigmoid(self.H1)) # (N, 2) .* (N, 2) -> (N, 2)\n    \n    self.dW1 = np.matmul(X.T, self.dA1) # (2, N) * (N, 2) -> (2, 2)\n    self.dB1 = np.sum(self.dA1, axis=0).reshape(1, -1) # (N, 2) -> (1, 2)\n\n      \n  def fit(self, X, Y, epochs=1, learning_rate=1, display_loss=False):\n      \n    if display_loss:\n      loss = {}\n    \n    for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n      self.grad(X, Y) # X -> (N, 2), Y -> (N, 4)\n        \n      m = X.shape[0]\n      self.W2 -= learning_rate * (self.dW2/m)\n      self.B2 -= learning_rate * (self.dB2/m)\n      self.W1 -= learning_rate * (self.dW1/m)\n      self.B1 -= learning_rate * (self.dB1/m)\n\n      if display_loss:\n        Y_pred = self.predict(X)\n        loss[i] = log_loss(np.argmax(Y, axis=1), Y_pred)\n    \n    \n    if display_loss:\n      plt.plot(np.fromiter(loss.values(),dtype=float))\n      plt.xlabel('Epochs')\n      plt.ylabel('Log Loss')\n      plt.show()\n      \n  \n  def predict(self, X):\n    Y_pred = self.forward_pass(X)\n    return np.array(Y_pred).squeeze()","execution_count":null,"outputs":[]},{"metadata":{"id":"VDe2wjl_ijH0","trusted":true},"cell_type":"code","source":"def read_all(folder_path, key_prefix=\"\"):\n    '''\n    It returns a dictionary with 'file names' as keys and 'flattened image arrays' as values.\n    '''\n    print(\"Reading images:\")\n    images = {}\n    files = os.listdir(folder_path)\n    for i, file_name in tqdm_notebook(enumerate(files), total=len(files)):\n        file_path = os.path.join(folder_path, file_name)\n        image_index = key_prefix + file_name[:-4]\n        image = Image.open(file_path)\n        image = image.convert(\"L\")\n        images[image_index] = np.array(image.copy()).flatten()\n        image.close()\n    return images","execution_count":null,"outputs":[]},{"metadata":{"id":"mjuaN532ijH4","outputId":"4124ae5e-4a9c-44dc-8c84-7919e6927fe5","trusted":true},"cell_type":"code","source":"languages = ['ta', 'hi', 'en']\n\nimages_train = read_all(\"../input/padhai-text-non-text-classification-level-1/level_1_train/\"+LEVEL+\"/\"+\"background\", key_prefix='bgr_') # change the path\nfor language in languages:\n  images_train.update(read_all(\"../input/padhai-text-non-text-classification-level-1/level_1_train/\"+LEVEL+\"/\"+language, key_prefix=language+\"_\" ))\nprint(len(images_train))\n\nimages_test = read_all(\"../input/padhai-text-non-text-classification-level-1/level_1_test/kaggle_\"+LEVEL, key_prefix='') # change the path\nprint(len(images_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"xqcTJRmSijH-","outputId":"3a26f608-868b-498d-b18d-bfae2b452d4e","trusted":true},"cell_type":"code","source":"list(images_test.keys())[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"yQUKxV_FijIC","outputId":"e6f2538b-3285-49ea-d6fd-7fbadc2bb975","trusted":true},"cell_type":"code","source":"X_train = []\nY_train = []\nfor key, value in images_train.items():\n    X_train.append(value)\n    if key[:4] == \"bgr_\":\n        Y_train.append(0)\n    else:\n        Y_train.append(1)\n\nID_test = []\nX_test = []\nfor key, value in images_test.items():\n  ID_test.append(int(key))\n  X_test.append(value)\n  \n        \nX_train = np.array(X_train)\nY_train = np.array(Y_train)\nX_test = np.array(X_test)\n\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"wy3IKx26ijIG","outputId":"f571f85d-1fe6-4a33-bcf1-ca1574aa3709","trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled_train = scaler.fit_transform(X_train)\nX_scaled_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W1 = np.random.randn(2,2)\nW2 = np.random.randn(2,4)\nprint(W1)\nprint(W2)","execution_count":null,"outputs":[]},{"metadata":{"id":"eboQW2n1ijIK","outputId":"fa8fbf5d-5d5c-4463-aa3c-909d6698b9b0","trusted":true},"cell_type":"code","source":"sn_mse = FF_MultiClass_InputWeightVectorised(W1,W2)\nsn_mse.fit(X_scaled_train, Y_train, epochs=250, learning_rate=0.5, display_loss=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"547SFsgsijIO","outputId":"e6595d5e-a9e0-4b5f-f7b5-a56297bc69c0","trusted":true},"cell_type":"code","source":"#sn_ce = FF_MultiClass_InputWeightVectorised(W1,W2)\n#sn_ce.fit(X_scaled_train, Y_train, epochs=250, learning_rate=0.001, loss_fn=\"ce\", display_loss=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"_a3_-9zYijIS","trusted":true},"cell_type":"code","source":"def print_accuracy(sn):\n  Y_pred_train = sn.predict(X_scaled_train)\n  Y_pred_binarised_train = (Y_pred_train >= 0.5).astype(\"int\").ravel()\n  accuracy_train = accuracy_score(Y_pred_binarised_train, Y_train)\n  print(\"Train Accuracy : \", accuracy_train)\n  print(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{"id":"lqe2g9PLijIW","outputId":"0ce4b45c-78f5-4323-829b-db3e12c3f268","trusted":true},"cell_type":"code","source":"print_accuracy(sn_mse)\n#print_accuracy(sn_ce)","execution_count":null,"outputs":[]},{"metadata":{"id":"8IMv7SCUijIa"},"cell_type":"markdown","source":"## Sample Submission"},{"metadata":{"id":"4_pBsgYlijIb","trusted":true},"cell_type":"code","source":"Y_pred_test = sn_ce.predict(X_scaled_test)\nY_pred_binarised_test = (Y_pred_test >= 0.5).astype(\"int\").ravel()\n\nsubmission = {}\nsubmission['ImageId'] = ID_test\nsubmission['Class'] = Y_pred_binarised_test\n\n\nsubmission = pd.DataFrame(submission)\nsubmission = submission[['ImageId', 'Class']]\ndf2 = pd.DataFrame({\"ImageId\":[19, 242,286], ##297rows -->300rows\n                    \"Class\":[1, 0, 0]})\nsubmission = submission.append(df2, ignore_index = True)\nsubmission = submission.sort_values(['ImageId'])\nsubmission.to_csv(\"submisision.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}